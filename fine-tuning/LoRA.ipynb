{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb396a87",
   "metadata": {},
   "source": [
    "- Step 1 â€” Loading LaMini-instruction dataset using load_dataset from huggingface\n",
    "- Step 2 â€” Loading Dolly Tokenizer and Model using huggingface (again!)\n",
    "- Step 3 â€” Data Preparation â€” Tokenize, split dataset and prepare for batch processing\n",
    "- Step 4 â€” Configuring LoRA and getting the PEFT model\n",
    "- Step 5 â€” Training the model and saving\n",
    "- Step 6 â€” Prediction with the finetuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebc4899",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4500051c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\GitHub_Clones\\ml-algo\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, List\n",
    "from datasets import Dataset, load_dataset, disable_caching \n",
    "disable_caching()\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5e2979",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07aeaa61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'List 5 reasons why someone should learn to code', 'response': '1. High demand for coding skills in the job market\\n2. Increased problem-solving and analytical skills\\n3. Ability to develop new products and technologies\\n4. Potentially higher earning potential\\n5. Opportunity to work remotely and/or freelance', 'instruction_source': 'alpaca'}\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('MBZUAI/LaMini-instruction', split='train')\n",
    "small_dataset = dataset.select(list(range(200)))\n",
    "print(small_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c627e3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<00:00, 6568.07 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'List 5 reasons why someone should learn to code', 'response': '1. High demand for coding skills in the job market\\n2. Increased problem-solving and analytical skills\\n3. Ability to develop new products and technologies\\n4. Potentially higher earning potential\\n5. Opportunity to work remotely and/or freelance', 'instruction_source': 'alpaca', 'prompt': 'Below is an instruction that describes a task. Write a response that appropriately completes the request. Instruction: List 5 reasons why someone should learn to code\\n Response:', 'answer': '1. High demand for coding skills in the job market\\n2. Increased problem-solving and analytical skills\\n3. Ability to develop new products and technologies\\n4. Potentially higher earning potential\\n5. Opportunity to work remotely and/or freelance', 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request. Instruction: List 5 reasons why someone should learn to code\\n Response:1. High demand for coding skills in the job market\\n2. Increased problem-solving and analytical skills\\n3. Ability to develop new products and technologies\\n4. Potentially higher earning potential\\n5. Opportunity to work remotely and/or freelance'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_template = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request. Instruction: {instruction}\\n Response:\"\"\"\n",
    "answer_template = \"\"\"{response}\"\"\"\n",
    "\n",
    "def _add_text(rec):\n",
    "    instruction = rec['instruction']\n",
    "    response = rec['response']\n",
    "    \n",
    "    if not instruction:\n",
    "        raise ValueError(f\"Expected an instruction in {rec}\")\n",
    "    if not response:\n",
    "        raise ValueError(f\"Expected a response in {rec}\")\n",
    "    \n",
    "    rec['prompt'] = prompt_template.format(instruction=instruction)\n",
    "    rec['answer'] = answer_template.format(response=response)\n",
    "    rec['text'] = rec['prompt'] + rec['answer']\n",
    "    return rec\n",
    "\n",
    "small_dataset = small_dataset.map(_add_text)\n",
    "print(small_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa26003",
   "metadata": {},
   "source": [
    "### Tokenizer and Model Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4928d8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_id = 'mistralai/Mistral-7B-v0.1'\n",
    "model_id = 'EleutherAI/pythia-2.8b'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4363ca90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 388/388 [00:02<00:00, 186.34it/s, Materializing param=gpt_neox.layers.31.post_attention_layernorm.weight] \n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    # load_in_8bit=True,\n",
    "    dtype=torch.float16,\n",
    ").to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02714ae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50277, 2560)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae246641",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c86ee78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<00:00, 2559.42 examples/s]\n",
      "Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<00:00, 2409.62 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "import copy\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "\n",
    "def _preprocess_batch(batch: Dict[str, List]):\n",
    "    model_inputs = tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    model_inputs[\"labels\"] = copy.deepcopy(model_inputs[\"input_ids\"])\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "_preprocessing_function = partial(_preprocess_batch)\n",
    "\n",
    "encoded_small_dataset = small_dataset.map(\n",
    "    _preprocessing_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"instruction\", \"response\", \"prompt\", \"answer\"],\n",
    ")\n",
    "processed_dataset = encoded_small_dataset.filter(\n",
    "    lambda rec: len(rec[\"input_ids\"]) <= MAX_LENGTH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f7a461f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['instruction_source', 'text', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 186\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['instruction_source', 'text', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 14\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "split_dataset = processed_dataset.train_test_split(test_size=14, seed=0)\n",
    "print(split_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db7bf60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a list of samples from a Dataset and collate them into a batch, as a dictionary of PyTorch tensors.\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_LENGTH,\n",
    "    pad_to_multiple_of=8,\n",
    "    padding=\"max_length\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f1777d",
   "metadata": {},
   "source": [
    "### Coniguring LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d320090f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 167,772,160 || all params: 2,942,842,880 || trainable%: 5.7010\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "LORA_R = 512\n",
    "LORA_ALPHA = 1024\n",
    "LORA_DROPOUT = 0.05\n",
    "TARGET_MODULES = [\"query_key_value\"]\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    bias=\"none\",\n",
    "    target_modules=TARGET_MODULES,\n",
    ")\n",
    "\n",
    "# model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a25d29",
   "metadata": {},
   "source": [
    "### Training Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4298e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"instruction_source\", \"text\", \"input_ids\", \"attention_mask\", \"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "20cda726",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora-pythia-2.8b\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=1,\n",
    "    fp16=True,\n",
    "    eval_strategy=\"epoch\",  # ðŸ‘ˆ important\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "721a4c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "\n",
    "    # Shift for causal LM\n",
    "    shift_logits = logits[..., :-1, :].reshape(-1, logits.shape[-1])\n",
    "    shift_labels = labels[..., 1:].reshape(-1)\n",
    "\n",
    "    # Compute loss manually\n",
    "    loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "    loss = loss_fct(\n",
    "        torch.tensor(shift_logits),\n",
    "        torch.tensor(shift_labels),\n",
    "    )\n",
    "\n",
    "    perplexity = math.exp(loss.item())\n",
    "    return {\n",
    "        \"eval_loss\": loss.item(),\n",
    "        \"perplexity\": perplexity,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "66981db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=split_dataset[\"train\"],\n",
    "    eval_dataset=split_dataset[\"test\"],\n",
    "    # compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "57d7f0b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 03:50, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.312354</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\GitHub_Clones\\ml-algo\\venv\\Lib\\site-packages\\peft\\utils\\save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6, training_loss=0.16954052448272705, metrics={'train_runtime': 259.5356, 'train_samples_per_second': 0.717, 'train_steps_per_second': 0.023, 'total_flos': 1607973517393920.0, 'train_loss': 0.16954052448272705, 'epoch': 1.0})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b75cce",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d17ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"lora-pythia-2.8b\")\n",
    "tokenizer.save_pretrained(\"lora-pythia-2.8b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68a5251",
   "metadata": {},
   "source": [
    "### loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8af7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, \"lora-pythia-2.8b\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
